# Мониторинг распределенных систем
## Определения
- *Мониторинг (наблюдение)*. Сбор, обработка, агрегирование и отображение в реальном времени количественных показателей системы.
- *Наблюдение методом белого ящика*. Наблюдение с использованием показателей, доступных внутри системы.
- *Наблюдение методом черного ящика*. Наблюдение и тестирование поведение, видимого извне, с точки зрения пользователя.
- *Информационная панель (панель управления) (дашборд)*. Приложение, которое предоставляет сводную информацию об основных показателях сервиса.
- *Оповещение (алерт)*. Сообщения, на которые должен обратить внимание человек. Делятся на "тикеты" (несрочные оповещения) и "вызовы" (экстренные оповещения).
- *Основная причина*. Дефект в ПО или ошибка, после исправления которой можно быть уверенным, что аналогичное событие не произойдёт.
- *Узел или машина*. Экземпляр запущенного ядра на физическом сервере, вм или контейнере. На одной машине может быть несколько сервисов, связанных или не связанных друг с другом.
- *Установка версии (push)*. Любые изменения, производимые в ПО работающего сервиса или его конфигурации
## Зачем нужен мониторинг
Оповещение, создание информационных панелей, сравнение с предыдущими версиями или экспериментальными группами, анализ долгосрочных тенденций, ретроспективный анализ (например отладка).
_Эффективные системы оповещения должны иметь хорошее соотношение "сигнал/шум". Вызов инженера - дорого. Частые ложные вызовы приводят к игнорированию настоящих._ 
## Ставим для мониторинга реальные задачи
...
## Симптомы и причины
Система мониторинга должна отвечать на два вопроса: что сломалось и почему. Ответ на вопрос "что?" - указание на симптом, "почему?" - указание на причину. Например: "сервис возвращает 500" - симптом, "серверы бд отказывают в соединении" - причина.
## Методы черного и белого ящика
Оповещение при наблюдении методом черного ящика обратит внимание только на реальные симптомы. Но для отладки ещё не произошедших, но неизбежных проблем он бесполезен - нужен метод белого ящика. 
## Четыре золотых сигнала
Время отклика, величина трафика, уровень ошибок и степень загруженности
## Позаботимся о своем "хвосте" (или производительность измеряемая и наблюдаемая)
Не стоит использовать средние значения для наблюдения. Гистограммы более информативны. Границы в гистограмме лучше расставлять по логорифму, например для времени отклика: 0-10мс,10-30,30-100,100-300 и т.д..
## Выбор подходящего уровня детализации для измерений
Не всегда нужна высокая детализация. Например, при оценке SLA/SLO. Если нужна высокая детализация, например метрик загрузки процессора, можно агрегировать их перед отправкой в систему мониторинга.
## Максимально просто, но не проще
Уровни сложности: оповещение о достижении разных порогов показателей; код обнаружения и отображения возможных причин; панели для каждой возможной причины.
С ростом сложности системы мониторинга, она становится неэффективной, сложно поддерживаемой и нестабильной. Чтобы избежать: правила определения аварий должны быть простыми, предсказуемыми и надежными; избавляться от редко или ограниченно используемых частей системы; от неиспользуемых показателей.
## Сводим все принципы воедино
Когда создаём триггер, задаемся вопросами:
1. Правило позволяет выявить **не обнаруживаемое иными средствами состояние*, которое требует быстрой реакции?
2. Смогу проигнорировать это оповещение, зная, что оно не критическое? Как и почему? Как избежать этого варианта?
3. Оповещение однозначно говорит о негативном влиянии на пользователя? Как распознать, когда нет?
4. Могу что-то предпринять в ответ на это оповещение? Немедлено или можно подождать до утра? Можно автоматизировать ответ? Решение будет временным или постоянным?
5. Придёт другим инженерам? Если да, какие-то из оповещений будут лишними

Правила реагирования на экстренные оповещения:
1. Каждый раз реагирую немедленно (могу так всего несколько раз в день)
2. Каждому оповещению соответствуют определенные ответные действия
3. Каждый ответ на оповещение должен быть продуманным. Если нечего ответить - оповещение не должно быть экстренным
4. Экстренные оповещения необходимы при появлении новых проблем
## Долгосрочное наблдение
**На примере BigTable**. Иногда, стоит снижать SLO, отключать оповещения и ограничивать работу с пользователями - чтобы сосредочиться на развитии системы.
**На примере Gmail**. Нужно сначала лечить костылями, потом, с облегчением - возвращать техдолг.
Контролируемое краткосрочное снижение показателей (SLO) обычно болезнено, но стратегически выгодно.
