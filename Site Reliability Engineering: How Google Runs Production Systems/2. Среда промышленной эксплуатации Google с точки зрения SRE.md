# Среда промышленной эксплуатации Google с точки зрения SRE
## Оборудование
Датацентры Google оснащены одинаково, однородны. *Машина* - единица оборудования, *сервер* - единица ПО. 
Машины размещаются в стойках, стойки стоят рядами, ряды образуют кластеры. Обычно в ДЦ несколько кластеров. Несколько зданий ДЦ составляют кампус.
Внутри ДЦ машины общаются через виртуальный коммутатор Jupiter. Между ДЦ проложена глобальная магистральная сеть B4

## Системное ПО, которое организует оборудование
### Управление машинами
Borg - распределенная система управления кластерами. Предшественник Kubernetes. Управляет заданиями на уровне кластеров. Отвечает за запуск заданий пользователей. 
Находит машины и запускает на них задания. Если задача работает некорректно - перезапускает, возможно на другой машине.
Так как задачи свободно распределяются между машинами, для обращения использется Borg Naming Service вместо IP-адресов.
Отвечает за выделение ресурсов и распределение задач по машинам.
### Хранилище
Разделено на несколько слоев: D - файловый сервер, работающий на всех машинах кластера; Colossus - кластерная файловая система; Базы данных (Bigtable, Spanner, ...)

**В**: Как обеспечивается работа внешних инструментов, не поддерживающих протокол Colossus?

### Сеть
Глобальный программный балансировщик нагрузки (GSLB) выполняет балансировку на трех уровнях: географическую, на уровне пользовательских сервисов (YouTube, Maps), на уровне удаленных вызовов процедур (RPC)

## Другое системное ПО
### Сервис блокировок
...
### Мониторинг и оповещение
Borgmon регулярно получает значения контрольных показателей. Могут использоваться немедленно для оповещения или сохранены для обработки и анализа: с целью настройки оповещений, сравнения поведения, планирования мощностей.

## Наша инфраструктура ПО
Весь код многопоточный, одна задача может использовать несколько ядер. Для поддержки дашбордов, мониторинга и отладки каждый сервер включает реализацию интерфейса предоставления диагностической информации и статистики по задачам.
Все сервисы общаются с помощью инфраструктуры удаленных вызовов процедур (RPC) Stubby (gRPC). Часто RPC используется даже для вызова подпрограмм в локальной программе - для большей модульности. Используют protobuf - он проще, компактнее и быстрее.

**В**: gRPC vs Kafka? \
_gRPC не предоставляет гарантий доставки. Kafka предоставляет + хранит данные. У gRPC меньше задержки и не требуется кластер между клиентом и сервисом_

**В**: В Google используют очереди типа Kafka, или только gRPC? 

## Наша среда разработки
Работают в общем репозитории. Если инженер сталкивает с проблемой в компоненте, за пределами своего проекта, он может её исправить. Весь код проходит ревью.
На каждое изменение запускаются тесты. Используют "отправку при успехе": прошли тесты - выложилось в прод.

**В**: Зачем действительно общий репозиторий? \
_Аргументация неубедитальна. Работая в отдельных репозиториях, так же могу принести PR соседней команде, если репозиторий открыт. Процесс ревью никак не зависит от общности репозитория. При этом, если речь о подтягивании исходников зависимостей, усложняется контроль изменений (помню несколько факапов по поводу) и иногда - замедляется сборка, теряется кросплатформенность (Snitch на маке не собрал, спасибо Cement)_ 

**В**: Какие могут быть причины не использовать "отправку при успехе" при условии прохождения ревью?

**В**: Работать над исправлением систем других команд в крупной компании - полезная практика?

## Shakespeare: пример сервиса
### Жизненный цикл запроса
Пользователь узнаёт IP-адрес подходящего GFE через DNS-сервер Google, после отсылает HTTP запрос GFE. Для выбора GFE, DNS-сервер взаимодействует с GSLB. GFE - обратный прокси. GFE находит, тоже через GSLB, подходящий экземляр сервиса, и перенаправляет запрос в него. Сервис, тоже через GSLB, находит подходящие подсистемы, получает от них результаты, и отдаёт GFE, после чего он отдаёт результат пользователю.

**В**: Обязательно весь трафик пропускать через GSLB? Какие плюсы/минусы такого подхода?

### Организация задач и данных
Принцип N+2: если для пиковой нагрузки достаточно N задач, нужно выделить N+2. Во время обновления одна будет временно недоступна - останется N+1. Во время сбоя в аппаратной части ещё одна - останется N.
Задачи распределяются соотвественно географии нагрузки. В каждом регионе по принципу N+2.

**В**: Все обновления возможно доставлять последовательно? Какие нельзя?



