# 2. Maintaining your Delta Lake
В этой главе рассмотрятся основные настройки и методы обслуживания, их влияние на производительность.
## Using Delta Lake Table Properties

 Property | Data Type | Use With | Default 
-|-|-|-
`delta.logRetentionDuration` | CalendarInterval | Cleaning | interval 30 days
`delta.deletedFileRetentionDuration` | CalendarInterval | Cleaning | interval 1 week
`delta.setTransactionRetentionDuration` | CalendarInterval | Cleaning, Repairing | (none)
`delta.targetFileSize`* | String | Tuning | (none)
`delta.tuneFileSizesForRewrites`* | Boolean | Tuning | (none)
`delta.autoOptimize.optimizeWrite`* | Boolean | Tuning | (none)
`delta.autoOptimize.autoCompact`* | Boolean | Tuning | (none)
`delta.dataSkippingNumIndexedCols` | Int | Tuning | 32
`delta.checkpoint.writeStatsAsStruct` | Boolean | Tuning | (none)
`delta.checkpoint.writeStatsAsJson` | Boolean | Tuning | true

_*свойства `targetFileSize`, `tuneFileSizesForRewrites`, `autoOptimize.optimizeWrite` и `autoOptimize.autoCompact` - доступны только в Databricks._

Метаданные, включая свойства таблицы хранятся рядом с ней. Свойства таблицы влияют только на метаданные и как правило не требуют физического изменения содержимого. Их можно менять без необходимости изменять код и перезапускать потоковые приложения.

### Create an Empty Table with Properties
```python
spark.sql("""
CREATE TABLE IF NOT EXISTS default.covid_nyt (
date DATE
) USING DELTA
TBLPROPERTIES('delta.logRetentionDuration'='interval 7 days');
""")
```
### Populate the Table
Пока таблица пустая и содержит только папку `_delta_log`. Проверить можно командой `spark.table("default.covid_nyt").inputFiles()`.

```python
from pyspark.sql.functions import to_date
(spark.read
  .format("parquet")
  .load("/opt/spark/work-dir/rs/data/COVID-19_NYT/*.parquet")
  .withColumn("date", to_date("date", "yyyy-MM-dd"))
  .write
  .format("delta")
  .saveAsTable("default.covid_nyt")
  )
```
Запись упадёт с исключением, не достаёт парраметра `.mode("append")`:
```
pyspark.sql.utils.AnalysisException: Table default.covid_nyt already exists
```
После его добавления запись всё равно будет падать с исключением из-за несовпадения схемы (Delta проверяет схему при записи):
```
pyspark.sql.utils.AnalysisException: A schema mismatch detected when writing to the Delta table (Table ID: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx)
```

Schema Enforcement - процесс проверки имеющейся схемы перед записью новых данных. Schema Evolution - процесс намеренного изменения схемы таблицы, обеспечивающий обратную совместимость.

### Evolve the Table Schema

Для автоматического применения новой схемы достаточно включить опцию `.option("mergeSchema", "true")`. Либо изменить таблицу вручную:
```python
spark.sql("""
ALTER TABLE default.covid_nyt
ADD COLUMNS (
county STRING,
state STRING,
fips INT,
cases INT,
deaths INT
);
""")
```
### Add or Modify Table Properties
```python
spark.sql("""
ALTER TABLE default.covid_nyc
SET TBLPROPERTIES (
'engineering.team_name'='dldg_authors',
'engineering.slack'='delta-users.slack.com'
)
""")
```

_Свойства таблицы могут содержать не только конфигурации, но и любую информацию. Например контакты владельца в Slack_

### Remove Table Properties
```python
spark.sql("""
ALTER TABLE default.covid_nyt
UNSET TBLPROPERTIES('delta.loRgetentionDuratio')
""")
```

Стандартный набор свойств таблиц можно указать в конфигурации spark `spark.delta.properties.defaults.<conf>`, например:
```
spark.delta.defaults.logRetentionDuration=interval 2 weeks
spark.delta.defaults.deletedFileRetentionDuration=interval 28 days
```

## Delta Table Optimization
### The Problem with Big Tables and Small Files
Маленькие файлы ухудшают производительность, особенно в распределенных системах, так как требуют большего числа операций ввода-вывода.
### Using Optimize to Fix the Small File Problem
#### Optimize
Optimize — служебная функция Delta, существует в двух вариантах: z-order и bin-packing. По умолчанию используется bin-packing - он объединяет множество маленьких файлов в меньшее количество больших файлов.

Конфигуация `spark.databricks.delta.optimize.minFileSize` задаёт порог минимального размера файла, после которого он будет обработан `OPTIMIZE`. Конфигурация `spark.databricks.delta.optimize.maxFileSize` задаёт максимальный размер файла, который может выдать `OPTIMIZE`. Конфигурация `spark.databricks.delta.optimize.repartition.enabled` указывает какую функцию использовать `OPTIMIZE`: `repartition` или `coalesce`

```python
results_df = (DeltaTable
    .forName(spark, "default.nonoptimal_covid_nyt")
    .optimize()
    .executeCompaction())
```

Проверить работу команды можно запросом:
```python
from pyspark.sql.functions import col
(
  DeltaTable.forName(spark, "default.nonoptimal_covid_nyt")
    .history(10)
    .where(col("operation") == "OPTIMIZE")
    .select("version", "timestamp", "operation", "operationMetrics.numRemovedFiles", "operationMetrics.numAddedFiles")
    .show(truncate=False)
)

# +-------+-----------------------+---------+---------------+-------------+
# |version|timestamp              |operation|numRemovedFiles|numAddedFiles|
# +-------+-----------------------+---------+---------------+-------------+
# |2      |2023-06-07 06:47:28.488|OPTIMIZE |9000           |1            |
# +-------+-----------------------+---------+---------------+-------------+
```

#### Z-Order Optimize
Z-Order — метод размещения связанной информации в одном наборе файлов. Локальность автоматически используется алгоритмами пропуска данных Delta Lake, что значительно уменьшает объем данных, которые необходимо прочитать.

## Table Tuning and Management
### Partitioning your Tables
Не стоит партиционировать таблицы меньше одного терабайта - достаточно использовать `OPTIMIZE` для контроля размера отдельных файлов. При выборе колонки для репартиционирования стоит убедиться, что каждая партиция будет больше хотябы гигабайта.
## Repairing, Restoring, and Replacing Table Data
### Recovering and Replacing Tables
### Deleting Data and Removing Partitions
### The Lifecycle of a Delta Lake Table
### Restoring your Table
Когда восстановить данные неоткуда, остаётся возможность вернутьсяк более ранней версии таблицы.
```python
dt = DeltaTable.forName(spark, "silver.covid_nyt_by_date")
  (dt.history(10)
  .select("version", "timestamp", "operation")
  .show())

# +-------+--------------------+--------------------+
# |version| timestamp          | operation          |
# +-------+--------------------+--------------------+
# | 1     |2023-06-09 19:11:...| DELETE             |
# | 0     |2023-06-09 19:04:...|CREATE TABLE AS S...|
# +-------+--------------------+--------------------+

dt.restoreToVersion(0)
```
### Cleaning Up
Удаление из таблиц Delta Lake не происходит немедленно. Фактически, сама операция просто удаляет ссылку из снимка таблицы Delta Lake, так что данные становятся невидимыми. Это означает, что у нас есть возможность отменить действие в случае случайного удаления данных. Реально удалить файлы можно с помощью операции vacuum

Команда `vacuum` очистит удаленные файлы или версии таблицы, которые больше не являются актуальными.
```
DeltaTable.forName(spark, "default.nonoptimal_covid_nyt").vacuum()
```

Команда vacuum не запустится сама по себе. Для автоматизации можно настроить задание cron для вызова vacuum с заданной частотой.

## Summary

_Пока не понял, как именно работает optimize - автоматически по записи, или только при прямом вызове вручную. Кажется, автоматизация из коробки доступна только в Databricks_