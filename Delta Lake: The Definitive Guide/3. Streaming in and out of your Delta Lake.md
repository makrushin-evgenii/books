# 3. Streaming in and out of your Delta Lake
## Streaming and Delta Lake
### Streaming vs Batch Processing
Разница между пакетной и потоковой обработкой в задержке: потоковая обрабатывает события при поступлении, пакетная пакетами. Source - источник, sink - назначение, checkpoint - контрольная точка.
### Delta as Source
Таблицу Delta можно использовать как источник: 
```python
streamingDeltaDf = spark.readStream.format("delta").option("ignoreDeletes", "true").load("/files/delta/user_events")
```
### Delta as Sink
```python
streamingDeltaDf.writeStream.format("delta").outputMode(“append”).start("/<delta_path>/")
```
## Delta streaming options
### Limit the Input Rate
`maxFilesPerTrigger` - ограничение количества файлов, обрабатываемых за одну итерацию. `maxBytesPerTrigger` - ограничение объема.
### Ignore Updates or Deletes
`ignoreDeletes` уберет из потока события удаления, `ignoreChanges` - изменения.
### Initial Processing Position
Используя таблицу Delta как источник, можно указать её версию `startingVersion` или время `startingTimestamp` с которого обрабатывать события. Нужную версию или время можно определить по логу транзакций:
```python
spark.readStream.format("delta").option("startingVersion", "5").load("/files/delta/user_events")
spark.readStream.format("delta").option("startingTimestamp", "2023-04-18").load("/files/delta/user_events")
```
### Initial Snapshot with EventTimeOrder
Стандартно порядок событий из таблицы Delta определяется временем модификации файлов. Опция `withEventTimeOrder` включает порядок по времени событий.
## Advanced Usage with Apache Spark
### Idempotent Stream Writes
Использование `foreachBatch` для записи в два назначения может привести к дублированию: если первая запись прошла успешно, а вторая упала - при перезапуске обработки микро-пачки запись в первое назначение повторится:
```python
def writeToDeltaLakeTables(batch_df):
    # location 1
    batch_df.write.format(“delta”).save("/<delta_path_1>/")
    # location 2
    batch_df.write.format(“delta”).save("/<delta_path_2>/")
# Apply the function against the micro-batches using ‘foreachBatch’
sourceDf.writeStream.format("delta").queryName("Unclear status stream").foreachBatch(writeToDeltaLakeTables).start()
```
Delta предоставляет две опции для идемпотентных записей: 
- `txnAppId` - уникальный идентификатор источника, передающийся в каждую операцию записи.
- `txnVersion` - монотонно увеличивающийся идентификатор, версия транзакции, смещение для запроса `writeStream`.

Использование двух этих идентификаторов вместе позволяет на уровне таблицы игнорировать попытки повторных записей:
```python
app_id = ... # A unique string used as an application ID.
def writeToDeltaLakeTableIdempotent(batch_df, batch_id):
    # location 1
    batch_df.write.format(“delta”).option("txnVersion", batch_id).option("txnAppId", app_id).save("/<delta_path>/")
    # location 2
    batch_df.write.format(“delta”).option("txnVersion", batch_id).option("txnAppId", app_id).save("/<delta_path>/")
```

Операция merge обычно используется для реализации upsert. Так как merge в таблице Delta требует двух проходов, использование в источнике недетрменированных функций типа `random` или `current_timestamp` приведет к некорректным результатам.

_Про merge нифига не понял, если честно. Надо будет перечитать, если столкнусь с необходимостью использовать upsert._
### Delta Lake Performance Metrics
Databrick при использовании Delta предоставляет метрики `numInputRows`, `inputRowsPerSecond`, и `processedRowsPerSecond`. Метрики обратного давления `numBytesOutstanding` и `numFilesOutstanding`.
## Auto Loader and Delta Live Tables
### Autoloader
Доступно только в Databricks, не стал читать.
### Delta Live Tables
Доступно только в Databricks, не стал читать.
## Change Data Feed
CDC - захват изменения данных. Delta поддерживает чтение собий изменения данных в таблице. 
### Using Change Data Feed 
Включить CDF (change data feed) можно через соответствующее свойство таблицы:
```
TBLPROPERTIES (delta.enableChangeDataFeed = true)
```
Для чтения CDF таблицы необходимо использовать опцию `readChangeFeed`. При этом так же доступны опции `startingVersion` и `startingTimestamp`:
```python
# providing a starting version
spark.readStream.format("delta").option("readChangeFeed", "true").option("startingVersion", 0).load("/pathToMyDeltaTable")
# providing a starting timestamp
spark.readStream.format("delta").option("readChangeFeed", "true").option("startingTimestamp", "2021-04-21 05:35:43").load("/pathToMyDeltaTable")
# not providing either
spark.readStream.format("delta").option("readChangeFeed", "true").load("/pathToMyDeltaTable")
```
### Schema
При чтении CDF мы получаем все колонки из таблицы + несколько служебных: `_change_type`, `_commit_version` и `_commit_timestamp`.